%%%%%%%% ICML 2026 SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[preprint]{icml2026}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2026}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

\newcommand{\don}[1]{\textcolor{blue}{[Dongfang: #1]}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[textsize=tiny]{todonotes}

\icmltitlerunning{Manifold-Consistent Graph Indexing}

\begin{document}

\twocolumn[
\icmltitle{Manifold-Consistent Graph Indexing: Overcoming the Euclidean-Geodesic Mismatch via Local Intrinsic Dimensionality}

\begin{icmlauthorlist}
% \icmlauthor{Anonymous Author}{equal}
\icmlauthor{Dongfang Zhao}{uw}
\end{icmlauthorlist}

\icmlaffiliation{uw}{University of Washington, Tacoma School of Engineering \& Technology and Paul G. Allen School of Computer Science \& Engineering}
% \icmlaffiliation{equal}{Anonymous Institution}

% \icmlcorrespondingauthor{Anonymous Author}{anon@example.com}
\icmlcorrespondingauthor{Dongfang Zhao}{dzhao@uw.edu}

\icmlkeywords{Approximate Nearest Neighbor Search, Graph Indexing, Retrieval-Augmented Generation, High-Dimensional Data}

\vskip 0.3in
]

\printAffiliationsAndNotice{} 

\begin{abstract}
Retrieval-augmented generation (RAG) and approximate nearest neighbor (ANN) search have been critical components of modern large language model (LLM) serving services as they enable efficient and effective retrieval of relevant information to reduce LLM's hallucination. 
However, state-of-the-art methods are mostly based on graph indexing techniques that are agnostic to the intrinsic geometry of the data, and thus often perform poorly in high-dimensional spaces due to a Euclidean-Geodesic mismatch.
To that end, we propose a new graph indexing method called Manifold-Consistent Graph Indexing (MCGI).
The key idea of MCGI is to leverage the local intrinsic dimensionality (LID) of the data to construct a graph that is consistent with the underlying manifold structure, thereby reducing the mismatch and improving performance.
Our theoretical analysis shows that MCGI achieves improved approximation guarantees comparing to existing methods, such as HNSW and DiskANN.
We also report experimental results demonstrating that MCGI outperforms existing methods in various benchmarks and real-world applications.
\end{abstract}

\section{Introduction}
\section{Related Work}

\section{Methodology} \label{method}

\subsection{Definitions}

We start by introducing the notions of Local Intrinsic Dimensionality (LID) in the words of analysis recently proposed by Houle~\cite{DBLP:conf/sisap/Houle17}.

\begin{definition}[Local Intrinsic Dimensionality]
\label{def:lid}
    Let $\mathcal{X}$ be a domain equipped with a distance measure $d: \mathcal{X} \times \mathcal{X} \to \mathbb{R}^+$. For a reference point $x \in \mathcal{X}$, let $F_x(r) = \mathbb{P}(d(x, Y) \le r)$ denote the cumulative distribution function (CDF) of the distance between $x$ and a random variable $Y$ drawn from the underlying data distribution. 
    The Local Intrinsic Dimensionality (LID) of $x$, denoted as $\text{ID}(x)$, is defined as the intrinsic growth rate of the probability measure within the neighborhood of $x$:
    \begin{equation}\label{eq:pid}
        \text{ID}(x) \triangleq \lim_{r \to 0} \frac{r \cdot F'_x(r)}{F_x(r)} = \lim_{r \to 0} \frac{d \ln F_x(r)}{d \ln r},
    \end{equation}
    provided the limit exists and $F_x(r)$ is continuously differentiable for $r > 0$.
\end{definition}

\begin{remark}[Institution of LID]
    The definition of LID can be understood as a measure of the multiplicative growth rate of the volume of a ball centered at $x$ with radius $r$ as $r$ approaches 0.
    Let $D$ denote the dimensionality of the ambient space.
    If the data lies on a local $D$-dimensional manifold, then the CDF around an infinitely small neighborhood of $x$ satisfies:
    \begin{equation}\label{eq:Fx}
        F_x(r) \approx C \cdot r^D,
    \end{equation}
    where $C$ is a constant.
    Thus, the following holds:
    \begin{equation}\label{eq:FxD}
        F'_x(r) \approx C \cdot D \cdot r^{D-1}.
    \end{equation}
    Combining equations~\eqref{eq:Fx} and~\eqref{eq:FxD}, we get:
    \begin{equation}
        D \approx \frac{F'_x(r)}{F_x(r)} \cdot r,
    \end{equation}
    thus Eq.~\eqref{eq:pid}.
\end{remark}

While Eq.~\refeq{def:lid} provides an intuitive closed-form formula for intrinsic dimensionality, in practice we usually do not have access to the true CDF $F_x(r)$.
Luckily, we can estimate LID from a finite sample of distances from $x$ to its neighbors using Maximum Likelihood Estimation (MLE) as proposed by~\cite{NIPS2004_74934548}.
According to~\cite{10.1145/2783258.2783405}, LID can be estimated as follows.
\begin{definition}[LID Maximum Likelihood Estimator]
\label{def:lid_mle}
Given a reference point $x$ and its $k$-nearest neighbors determined by the distance measure $d$, let $r_i = d(x, v_i)$ denote the distance to the $i$-th nearest neighbor, sorted such that $r_1 \le \dots \le r_k$. 
Following the formulation in~\cite{10.1145/2783258.2783405}, which adapts the Hill estimator for intrinsic dimensionality, the LID at $x$ is estimated as:
\begin{equation}
    \widehat{\text{LID}}(x) = - \left( \frac{1}{k} \sum_{i=1}^{k} \ln \frac{r_i}{r_k} \right)^{-1}.
\end{equation}
\end{definition}

\subsection{Mapping Function}
\label{subsec:mapping}

The primary goal of Manifold-Consistent Graph Indexing is that the graph topology should adapt to the local geometric complexity. 
In regions where the Local Intrinsic Dimensionality (LID) is low, the data manifold approximates a flat Euclidean subspace. In such isotropic regions, the Euclidean metric is a reliable proxy for geodesic distance, allowing for aggressive edge pruning (larger $\alpha$) to permit long-range ``highway'' connections without risking semantic shortcuts.
Conversely, regions with high LID typically exhibit significant curvature, noise, or singularity. Here, the Euclidean distance often violates the manifold geodesic structure. To preserve topological fidelity, the indexing algorithm must adopt a conservative pruning strategy (smaller $\alpha$), thereby forcing the search to take smaller, safer steps along the manifold surface.

Let $u \in V$ be a node in the graph, and $\widehat{\text{LID}}(u)$ be its estimated local intrinsic dimensionality. We define the pruning parameter $\alpha(u)$ as:
\begin{equation}
    \alpha(u) \triangleq \Phi( \widehat{\text{LID}}(u) ).
\end{equation}
The function $\Phi: \mathbb{R}^+ \to [\alpha_{\min}, \alpha_{\max}]$ is designed to satisfy the following geometric intuition: in regions with high LID, the graph should enforce a stricter connectivity constraint (smaller $\alpha$) to avoid short-circuiting the manifold; conversely, in low-LID regions, the constraint can be relaxed (larger $\alpha$).

To ensure the mapping is robust across datasets with varying complexity scales, we employ Z-score normalization based on the empirical distribution of the LID estimates. We first compute the normalized score $z(u)$:
\begin{equation}
    z(u) = \frac{\widehat{\text{LID}}(u) - \mu_{\widehat{\text{LID}}}}{\sigma_{\widehat{\text{LID}}}},
\end{equation}
where $\mu_{\widehat{\text{LID}}}$ and $\sigma_{\widehat{\text{LID}}}$ denote the mean and standard deviation of the set of estimated LID values $\{ \widehat{\text{LID}}(v) \mid v \in V \}$ computed across the entire graph.

We then formulate $\Phi$ using a logistic function to smoothly map the Z-score to the operational range $[\alpha_{\min}, \alpha_{\max}]$:
\begin{equation}
    \Phi(\widehat{\text{LID}}(u)) = \alpha_{\min} + \frac{\alpha_{\max} - \alpha_{\min}}{1 + \exp(z(u))}.
\end{equation}
We set $\alpha_{\min}=1.0$ and $\alpha_{\max}=1.5$ following standard practices in graph indexing~\cite{jayaram2019diskann}. 
This formulation ensures that nodes with average complexity ($z(u) \approx 0$) are assigned $\alpha \approx 1.25$, while nodes with significantly higher complexity ($z(u) > 0$) are penalized with a stricter $\alpha$ approaching the limit of 1.0.

The mapping function $\Phi$ satisfies the following geometric properties essential for stable graph construction: Monotonicity and Boundedness.

\begin{proposition}[Monotonicity]
The mapping function $\Phi$ is strictly decreasing with respect to the estimated local intrinsic dimensionality. Formally, given that the standard deviation of the LID estimates $\sigma_{\widehat{\text{LID}}} > 0$ and the pruning range $\alpha_{\max} > \alpha_{\min}$, the derivative satisfies:
\begin{equation}
    \frac{d \Phi}{d \widehat{\text{LID}}(u)} < 0.
\end{equation}
\end{proposition}

\begin{proof}
Let $L = \widehat{\text{LID}}(u)$ be the independent variable. We define the normalized Z-score $z$ as a function of $L$:
\begin{equation}
    z(L) = \frac{L - \mu_{\widehat{\text{LID}}}}{\sigma_{\widehat{\text{LID}}}}.
\end{equation}
The mapping function is defined as:
\begin{equation}
    \Phi(L) = \alpha_{\min} + \frac{C}{1 + \exp(z(L))},
\end{equation}
where $C = \alpha_{\max} - \alpha_{\min}$. Since we strictly set $\alpha_{\max} = 1.5$ and $\alpha_{\min} = 1.0$, it follows that $C > 0$.
To determine the sign of the gradient, we apply the chain rule:
\begin{equation}
    \frac{d \Phi}{d L} = \frac{d \Phi}{d z} \cdot \frac{d z}{d L}.
\end{equation}
First, we differentiate the Z-score term with respect to $L$:
\begin{equation}
    \frac{d z}{d L} = \frac{1}{\sigma_{\widehat{\text{LID}}}}.
\end{equation}
Next, we differentiate the logistic component $\Phi$ with respect to $z$:
\begin{align}
    \frac{d \Phi}{d z} &= \frac{d}{d z} \left( \alpha_{\min} + C (1 + e^z)^{-1} \right) \\
    &= C \cdot (-1) \cdot (1 + e^z)^{-2} \cdot \frac{d}{d z}(1 + e^z) \\
    &= - C \cdot \frac{e^z}{(1 + e^z)^2}.
\end{align}
Combining these terms yields the full derivative:
\begin{equation}
    \frac{d \Phi}{d L} = - \frac{C}{\sigma_{\widehat{\text{LID}}}} \cdot \frac{e^z}{(1 + e^z)^2}.
\end{equation}
We analyze the sign of each component:
\begin{itemize}
    \item The operational range constant $C > 0$.
    \item The standard deviation $\sigma_{\widehat{\text{LID}}} > 0$, assuming the dataset exhibits non-zero geometric variance.
    \item The exponential function $e^z > 0$ for all $z \in \mathbb{R}$.
    \item The denominator $(1 + e^z)^2 > 0$.
\end{itemize}
Therefore, the term $\frac{C}{\sigma_{\widehat{\text{LID}}}} \frac{e^z}{(1 + e^z)^2}$ is strictly positive. The leading negative sign guarantees that $\frac{d \Phi}{d L} < 0$.
This confirms that the pruning parameter $\alpha$ strictly decreases as the local geometric complexity increases, thereby enforcing a more conservative graph topology in high-LID regions.
\end{proof}

\begin{proposition}[Boundedness]
The pruning parameter $\alpha(u)$ derived from the mapping function is strictly bounded within the prescribed operational interval. For any node $u$ with a finite LID estimate:
\begin{equation}
    \alpha_{\min} < \alpha(u) < \alpha_{\max}.
\end{equation}
\end{proposition}

\begin{proof}
Let $S(u)$ denote the logistic component of the mapping function:
\begin{equation}
    S(u) = \frac{1}{1 + \exp(z(u))}.
\end{equation}
For any finite input $\widehat{\text{LID}}(u)$, the Z-score $z(u)$ is finite. The exponential function maps the real line to the positive real line, i.e., $\exp(z(u)) \in (0, \infty)$.
Consequently, the denominator lies in the interval $(1, \infty)$. Taking the reciprocal yields the bounds for the logistic component:
\begin{equation}
    0 < S(u) < 1.
\end{equation}
Substituting $S(u)$ back into the definition of $\Phi$:
\begin{equation}
    \alpha(u) = \alpha_{\min} + (\alpha_{\max} - \alpha_{\min}) \cdot S(u).
\end{equation}
Since $(\alpha_{\max} - \alpha_{\min}) > 0$, we can apply the inequality boundaries:
\begin{align}
    \alpha(u) &> \alpha_{\min} + (\alpha_{\max} - \alpha_{\min}) \cdot 0 = \alpha_{\min}, \\
    \alpha(u) &< \alpha_{\min} + (\alpha_{\max} - \alpha_{\min}) \cdot 1 = \alpha_{\max}.
\end{align}
This proves that the topology is strictly confined. 
The pruning behavior never exceeds the relaxation upper limit ($\alpha_{\max}$) and never becomes stricter than the lower limit ($\alpha_{\min}$), ensuring graph connectivity and preventing degree explosion.
\end{proof}

\subsection{Algorithm}
\label{subsec:algorithm}

The MCGI algorithm (Algorithm~\ref{alg:mcgi}) alters the standard graph refinement pipeline by introducing a geometric calibration phase. Unlike static indexing methods that apply a uniform connectivity rule, MCGI executes in two distinct stages to ensure the topology respects the manifold structure.

\begin{algorithm}[tb]
   \caption{Manifold-Consistent Graph Indexing (MCGI)}
   \label{alg:mcgi}
\begin{algorithmic}
   \STATE Input: Dataset $X$, Max Degree $R$, Beam Width $L$
   \STATE Output: Optimized Graph $G$
   
   \STATE \COMMENT{Phase 1: Geometric Calibration}
   \STATE $\mathcal{L} \leftarrow \text{ParallelEstimateLID}(X)$
   \STATE $\mu \leftarrow \text{Mean}(\mathcal{L})$
   \STATE $\sigma \leftarrow \text{StdDev}(\mathcal{L})$

   \FOR{each node $u \in V$ in parallel}
       \STATE $z_u \leftarrow (\mathcal{L}[u] - \mu) / \sigma$
       \STATE $\alpha_u \leftarrow \alpha_{\min} + (\alpha_{\max} - \alpha_{\min}) / (1 + \exp(z_u))$
   \ENDFOR
   
   \STATE \COMMENT{Phase 2: Topology Refinement}
   \STATE $G \leftarrow \text{RandomGraph}(X, R)$
   
   \FOR{$iter \leftarrow 1$ to $MaxIter$}
       \FOR{each node $u \in G$ in parallel}
           \STATE $\mathcal{C} \leftarrow \text{GreedySearch}(u, G, L)$
           \STATE $\mathcal{N}_{new} \leftarrow \emptyset$
           
           \FOR{$v \in \text{SortByDistance}(\mathcal{C} \cup \mathcal{N}(u))$}
               \STATE $pruned \leftarrow \text{False}$
               \FOR{$n \in \mathcal{N}_{new}$}
                   \IF{$\alpha_u \cdot d(n, v) \le d(u, v)$}
                       \STATE $pruned \leftarrow \text{True}$; break
                   \ENDIF
               \ENDFOR
               \IF{not $pruned$ \AND $|\mathcal{N}_{new}| < R$}
                   \STATE $\mathcal{N}_{new}.\text{add}(v)$
               \ENDIF
           \ENDFOR
           \STATE $\mathcal{N}(u) \leftarrow \mathcal{N}_{new}$
       \ENDFOR
   \ENDFOR
\end{algorithmic}
\end{algorithm}

\paragraph{Phase 1: Geometric Calibration.}
Before modifying the graph topology, the system first performs a global analysis of the dataset geometry. We estimate the LID for every point and aggregate the population statistics ($\mu, \sigma$) defined in Section~\ref{subsec:mapping}. 
This phase ``freezes'' the geometric profile of the dataset. By pre-computing these statistics, we decouple the complexity estimation from the graph update loop, ensuring that the mapping function $\Phi$ remains stable and computationally efficient during the intensive edge-selection process.

\paragraph{Phase 2: Manifold-Consistent Refinement.}
The index construction follows an iterative refinement strategy. 
Let $\mathcal{N}(u)$ denote the set of neighbors for node $u$ in the graph $G$. 
In each iteration, the algorithm dynamically updates $\mathcal{N}(u)$ by:
\begin{enumerate}
    \item Queries the pre-computed geometric profile to determine the node-specific constraint $\alpha(u)$.
    \item Explores the graph to identify a candidate pool $\mathcal{C}$.
    \item Filters connections using the dynamic occlusion criterion. 
\end{enumerate}

\subsection{Theoretical Analysis}
\label{subsec:analysis}

We analyze the proposed method regarding its computational overhead and topological guarantees.

\paragraph{Computational Complexity.}

The MCGI construction adds a pre-processing step (Phase 1) but maintains the same asymptotic complexity as standard graph construction algorithms (Phase 2).
\begin{itemize}
    \item \textit{Calibration Overhead:} The LID estimation requires a $k$-nearest neighbor search for a subset of points. Assuming a sample size of $N$ and a fixed reference set size, this operation is bounded by $O(N \log N)$. The subsequent Z-score calculation and $\alpha$ mapping are linear scan operations, $O(N)$.
    \item \textit{Construction Cost:} The core refinement loop (Algorithm 1, Phase 2) has a complexity of $O(T \cdot N \cdot R \cdot \log L)$, where $T$ is the number of iterations and $R$ is the degree bound. The node-specific parameter $\alpha(u)$ is accessed via a constant-time lookup $O(1)$ during the edge pruning check.
\end{itemize}
Since the calibration is a non-iterative, one-time operation, it does not alter the asymptotic complexity class of the indexing pipeline. 
The total time complexity remains dominated by the iterative distance computations required for edge selection, ensuring that the method scales linearly with $N$ similar to baseline iterative algorithms.

\paragraph{Connectivity Preservation.}
A primary concern with dynamic pruning is whether strict constraints fragment the graph into disconnected components.
To address this, we observe that the strictest constraint applied by our algorithm (where $\alpha(u) \to 1.0$) corresponds to the definition of the Relative Neighborhood Graph (RNG).
It has been theoretically established that the RNG is a supergraph of the Euclidean Minimum Spanning Tree (EMST) for any set of points in general position~\cite{TOUSSAINT1980261}.  
Since the EMST is connected, the RNG is necessarily connected.
In our method, the pruning parameter satisfies $\alpha(u) \ge 1.0$ for all $u$. Let $E_{MCGI}$ be the edge set produced by our algorithm and $E_{RNG}$ be the edge set of the exact RNG. The condition implies:
\begin{equation}
    E_{RNG} \subseteq E_{MCGI}.
\end{equation}
Thus, provided that the greedy search sufficiently explores the candidate space, the resulting topology retains the connectivity guarantees of the RNG, ensuring that every node remains reachable from the entry point.

\section{Evaluation}
\section{Conclusion}


\nocite{*}
\bibliography{ref}
\bibliographystyle{icml2026}

\end{document}