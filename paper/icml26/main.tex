%%%%%%%% ICML 2026 SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[preprint]{icml2026}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2026}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

\newcommand{\don}[1]{\textcolor{blue}{[Dongfang: #1]}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[textsize=tiny]{todonotes}

\icmltitlerunning{Manifold-Consistent Graph Indexing}

\begin{document}

\twocolumn[
\icmltitle{Manifold-Consistent Graph Indexing: Overcoming the Euclidean-Geodesic Mismatch via Local Intrinsic Dimensionality}

\begin{icmlauthorlist}
% \icmlauthor{Anonymous Author}{equal}
\icmlauthor{Dongfang Zhao}{uw}
\end{icmlauthorlist}

\icmlaffiliation{uw}{University of Washington, Tacoma School of Engineering \& Technology and Paul G. Allen School of Computer Science \& Engineering}
% \icmlaffiliation{equal}{Anonymous Institution}

% \icmlcorrespondingauthor{Anonymous Author}{anon@example.com}
\icmlcorrespondingauthor{Dongfang Zhao}{dzhao@uw.edu}

\icmlkeywords{Approximate Nearest Neighbor Search, Graph Indexing, Retrieval-Augmented Generation, High-Dimensional Data}

\vskip 0.3in
]

\printAffiliationsAndNotice{} 

\begin{abstract}
Retrieval-augmented generation (RAG) and approximate nearest neighbor (ANN) search have been critical components of modern large language model (LLM) serving services as they enable efficient and effective retrieval of relevant information to reduce LLM's hallucination. 
However, state-of-the-art methods are mostly based on graph indexing techniques that are agnostic to the intrinsic geometry of the data, and thus often perform poorly in high-dimensional spaces due to a Euclidean-Geodesic mismatch.
To that end, we propose a new graph indexing method called Manifold-Consistent Graph Indexing (MCGI).
The key idea of MCGI is to leverage the local intrinsic dimensionality (LID) of the data to construct a graph that is consistent with the underlying manifold structure, thereby reducing the mismatch and improving performance.
Our theoretical analysis shows that MCGI achieves improved approximation guarantees comparing to existing methods, such as HNSW and DiskANN.
We also report experimental results demonstrating that MCGI outperforms existing methods in various benchmarks and real-world applications.
\end{abstract}

\section{Introduction}
\section{Related Work}

\section{Methodology} \label{method}

\subsection{Definitions}

We start by introducing the notions of Local Intrinsic Dimensionality (LID) in the words of analysis recently proposed by Houle~\cite{DBLP:conf/sisap/Houle17}.

\begin{definition}[Local Intrinsic Dimensionality]
\label{def:lid}
    Let $\mathcal{X}$ be a domain equipped with a distance measure $d: \mathcal{X} \times \mathcal{X} \to \mathbb{R}^+$. For a reference point $x \in \mathcal{X}$, let $F_x(r) = \mathbb{P}(d(x, Y) \le r)$ denote the cumulative distribution function (CDF) of the distance between $x$ and a random variable $Y$ drawn from the underlying data distribution. 
    The Local Intrinsic Dimensionality (LID) of $x$, denoted as $\text{ID}(x)$, is defined as the intrinsic growth rate of the probability measure within the neighborhood of $x$:
    \begin{equation}\label{eq:pid}
        \text{ID}(x) \triangleq \lim_{r \to 0} \frac{r \cdot F'_x(r)}{F_x(r)} = \lim_{r \to 0} \frac{d \ln F_x(r)}{d \ln r},
    \end{equation}
    provided the limit exists and $F_x(r)$ is continuously differentiable for $r > 0$.
\end{definition}

\begin{remark}[Institution of LID]
    The definition of LID can be understood as a measure of the multiplicative growth rate of the volume of a ball centered at $x$ with radius $r$ as $r$ approaches 0.
    Let $D$ denote the dimensionality of the ambient space.
    If the data lies on a local $D$-dimensional manifold, then the CDF around an infinitely small neighborhood of $x$ satisfies:
    \begin{equation}\label{eq:Fx}
        F_x(r) \approx C \cdot r^D,
    \end{equation}
    where $C$ is a constant.
    Thus, the following holds:
    \begin{equation}\label{eq:FxD}
        F'_x(r) \approx C \cdot D \cdot r^{D-1}.
    \end{equation}
    Combining equations~\eqref{eq:Fx} and~\eqref{eq:FxD}, we get:
    \begin{equation}
        D \approx \frac{F'_x(r)}{F_x(r)} \cdot r,
    \end{equation}
    thus Eq.~\eqref{eq:pid}.
\end{remark}

While Eq.~\refeq{def:lid} provides an intuitive closed-form formula for intrinsic dimensionality, in practice we usually do not have access to the true CDF $F_x(r)$.
Luckily, we can estimate LID from a finite sample of distances from $x$ to its neighbors using Maximum Likelihood Estimation (MLE) as proposed by~\cite{NIPS2004_74934548}.
According to~\cite{10.1145/2783258.2783405}, LID can be estimated as follows.
\begin{definition}[LID Maximum Likelihood Estimator]
\label{def:lid_mle}
Given a reference point $x$ and its $k$-nearest neighbors determined by the distance measure $d$, let $r_i = d(x, v_i)$ denote the distance to the $i$-th nearest neighbor, sorted such that $r_1 \le \dots \le r_k$. 
Following the formulation in~\cite{10.1145/2783258.2783405}, which adapts the Hill estimator for intrinsic dimensionality, the LID at $x$ is estimated as:
\begin{equation}
    \widehat{\text{LID}}(x) = - \left( \frac{1}{k} \sum_{i=1}^{k} \ln \frac{r_i}{r_k} \right)^{-1}.
\end{equation}
\end{definition}

\subsection{Mapping Function}
\label{subsec:mapping}

The primary goal of Manifold-Consistent Graph Indexing is that the graph topology should adapt to the local geometric complexity. 
In regions where the Local Intrinsic Dimensionality (LID) is low, the data manifold approximates a flat Euclidean subspace. In such isotropic regions, the Euclidean metric is a reliable proxy for geodesic distance, allowing for aggressive edge pruning (larger $\alpha$) to permit long-range ``highway'' connections without risking semantic shortcuts.
Conversely, regions with high LID typically exhibit significant curvature, noise, or singularity. Here, the Euclidean distance often violates the manifold geodesic structure. To preserve topological fidelity, the indexing algorithm must adopt a conservative pruning strategy (smaller $\alpha$), thereby forcing the search to take smaller, safer steps along the manifold surface.

Let $u \in V$ be a node in the graph, and $\widehat{\text{LID}}(u)$ be its estimated local intrinsic dimensionality. We define the pruning parameter $\alpha(u)$ as:
\begin{equation}
    \alpha(u) \triangleq \Phi( \widehat{\text{LID}}(u) ).
\end{equation}
The function $\Phi: \mathbb{R}^+ \to [\alpha_{\min}, \alpha_{\max}]$ is designed to satisfy the following geometric intuition: in regions with high LID, the graph should enforce a stricter connectivity constraint (smaller $\alpha$) to avoid short-circuiting the manifold; conversely, in low-LID regions, the constraint can be relaxed (larger $\alpha$).

To ensure the mapping is robust across datasets with varying complexity scales, we employ Z-score normalization based on the empirical distribution of the LID estimates. We first compute the normalized score $z(u)$:
\begin{equation}
    z(u) = \frac{\widehat{\text{LID}}(u) - \mu_{\widehat{\text{LID}}}}{\sigma_{\widehat{\text{LID}}}},
\end{equation}
where $\mu_{\widehat{\text{LID}}}$ and $\sigma_{\widehat{\text{LID}}}$ denote the mean and standard deviation of the set of estimated LID values $\{ \widehat{\text{LID}}(v) \mid v \in V \}$ computed across the entire graph.

We then formulate $\Phi$ using a logistic function to smoothly map the Z-score to the operational range $[\alpha_{\min}, \alpha_{\max}]$:
\begin{equation}
    \Phi(\widehat{\text{LID}}(u)) = \alpha_{\min} + \frac{\alpha_{\max} - \alpha_{\min}}{1 + \exp(z(u))}.
\end{equation}
We employ the logistic function over a linear mapping to exploit its saturation properties. 
LID estimates often exhibit heavy-tailed distributions with extreme outliers. 
A linear mapping would be hypersensitive to these outliers, skewing the $\alpha$ values for the majority of the data. 
The logistic function acts as a robust soft-thresholding mechanism: it reduces the variance in the high-LID and low-LID tails (saturating towards $\alpha_{\min}$ and $\alpha_{\max}$, respectively) while maintaining sensitivity in the transition region around the population mean.
We set $\alpha_{\min}=1.0$ and $\alpha_{\max}=1.5$ following standard practices in graph indexing~\cite{jayaram2019diskann}. 
This formulation ensures that nodes with average complexity ($z(u) \approx 0$) are assigned $\alpha \approx 1.25$, while nodes with significantly higher complexity ($z(u) > 0$) are penalized with a stricter $\alpha$ approaching the limit of 1.0.

The mapping function $\Phi$ satisfies the following geometric properties essential for stable graph construction: Monotonicity and Boundedness.

\begin{proposition}[Monotonicity]
The mapping function $\Phi$ is strictly decreasing with respect to the estimated local intrinsic dimensionality. Formally, given that the standard deviation of the LID estimates $\sigma_{\widehat{\text{LID}}} > 0$ and the pruning range $\alpha_{\max} > \alpha_{\min}$, the derivative satisfies:
\begin{equation}
    \frac{d \Phi}{d \widehat{\text{LID}}(u)} < 0.
\end{equation}
\end{proposition}

\begin{proof}
Let $L = \widehat{\text{LID}}(u)$ be the independent variable. We define the normalized Z-score $z$ as a function of $L$:
\begin{equation}
    z(L) = \frac{L - \mu_{\widehat{\text{LID}}}}{\sigma_{\widehat{\text{LID}}}}.
\end{equation}
The mapping function is defined as:
\begin{equation}
    \Phi(L) = \alpha_{\min} + \frac{C}{1 + \exp(z(L))},
\end{equation}
where $C = \alpha_{\max} - \alpha_{\min}$. Since we strictly set $\alpha_{\max} = 1.5$ and $\alpha_{\min} = 1.0$, it follows that $C > 0$.
To determine the sign of the gradient, we apply the chain rule:
\begin{equation}
    \frac{d \Phi}{d L} = \frac{d \Phi}{d z} \cdot \frac{d z}{d L}.
\end{equation}
First, we differentiate the Z-score term with respect to $L$:
\begin{equation}
    \frac{d z}{d L} = \frac{1}{\sigma_{\widehat{\text{LID}}}}.
\end{equation}
Next, we differentiate the logistic component $\Phi$ with respect to $z$:
\begin{align}
    \frac{d \Phi}{d z} &= \frac{d}{d z} \left( \alpha_{\min} + C (1 + e^z)^{-1} \right) \\
    &= C \cdot (-1) \cdot (1 + e^z)^{-2} \cdot \frac{d}{d z}(1 + e^z) \\
    &= - C \cdot \frac{e^z}{(1 + e^z)^2}.
\end{align}
Combining these terms yields the full derivative:
\begin{equation}
    \frac{d \Phi}{d L} = - \frac{C}{\sigma_{\widehat{\text{LID}}}} \cdot \frac{e^z}{(1 + e^z)^2}.
\end{equation}
We analyze the sign of each component:
\begin{itemize}
    \item The operational range constant $C > 0$.
    \item The standard deviation $\sigma_{\widehat{\text{LID}}} > 0$, assuming the dataset exhibits non-zero geometric variance.
    \item The exponential function $e^z > 0$ for all $z \in \mathbb{R}$.
    \item The denominator $(1 + e^z)^2 > 0$.
\end{itemize}
Therefore, the term $\frac{C}{\sigma_{\widehat{\text{LID}}}} \frac{e^z}{(1 + e^z)^2}$ is strictly positive. The leading negative sign guarantees that $\frac{d \Phi}{d L} < 0$.
This confirms that the pruning parameter $\alpha$ strictly decreases as the local geometric complexity increases, thereby enforcing a more conservative graph topology in high-LID regions.
\end{proof}

\begin{proposition}[Boundedness]
The pruning parameter $\alpha(u)$ derived from the mapping function is strictly bounded within the prescribed operational interval. For any node $u$ with a finite LID estimate:
\begin{equation}
    \alpha_{\min} < \alpha(u) < \alpha_{\max}.
\end{equation}
\end{proposition}

\begin{proof}
Let $S(u)$ denote the logistic component of the mapping function:
\begin{equation}
    S(u) = \frac{1}{1 + \exp(z(u))}.
\end{equation}
For any finite input $\widehat{\text{LID}}(u)$, the Z-score $z(u)$ is finite. The exponential function maps the real line to the positive real line, i.e., $\exp(z(u)) \in (0, \infty)$.
Consequently, the denominator lies in the interval $(1, \infty)$. Taking the reciprocal yields the bounds for the logistic component:
\begin{equation}
    0 < S(u) < 1.
\end{equation}
Substituting $S(u)$ back into the definition of $\Phi$:
\begin{equation}
    \alpha(u) = \alpha_{\min} + (\alpha_{\max} - \alpha_{\min}) \cdot S(u).
\end{equation}
Since $(\alpha_{\max} - \alpha_{\min}) > 0$, we can apply the inequality boundaries:
\begin{align}
    \alpha(u) &> \alpha_{\min} + (\alpha_{\max} - \alpha_{\min}) \cdot 0 = \alpha_{\min}, \\
    \alpha(u) &< \alpha_{\min} + (\alpha_{\max} - \alpha_{\min}) \cdot 1 = \alpha_{\max}.
\end{align}
This proves that the topology is strictly confined. 
The pruning behavior never exceeds the relaxation upper limit ($\alpha_{\max}$) and never becomes stricter than the lower limit ($\alpha_{\min}$), ensuring graph connectivity and preventing degree explosion.
\end{proof}

\subsection{Manifold-Consistent Graph Indexing}
\label{subsec:algorithm}

The MCGI algorithm (Algorithm~\ref{alg:mcgi}) alters the standard graph refinement pipeline by introducing a geometric calibration phase. Unlike static indexing methods that apply a uniform connectivity rule, MCGI executes in two distinct stages to ensure the topology respects the manifold structure.

\begin{algorithm}[tb]
   \caption{Manifold-Consistent Graph Indexing (MCGI)}
   \label{alg:mcgi}
\begin{algorithmic}
   \STATE Input: Dataset $X$, Max Degree $R$, Beam Width $L$
   \STATE Output: Optimized Graph $G$
   
   \STATE \COMMENT{Phase 1: Geometric Calibration}
   \STATE $\mathcal{L} \leftarrow \text{ParallelEstimateLID}(X)$
   \STATE $\mu \leftarrow \text{Mean}(\mathcal{L})$
   \STATE $\sigma \leftarrow \text{StdDev}(\mathcal{L})$

   \FOR{each node $u \in V$ in parallel}
       \STATE $z_u \leftarrow (\mathcal{L}[u] - \mu) / \sigma$
       \STATE $\alpha_u \leftarrow \alpha_{\min} + (\alpha_{\max} - \alpha_{\min}) / (1 + \exp(z_u))$
   \ENDFOR
   
   \STATE \COMMENT{Phase 2: Topology Refinement}
   \STATE $G \leftarrow \text{RandomGraph}(X, R)$
   
   \FOR{$iter \leftarrow 1$ to $MaxIter$}
       \FOR{each node $u \in G$ in parallel}
           \STATE $\mathcal{C} \leftarrow \text{GreedySearch}(u, G, L)$
           \STATE $\mathcal{N}_{new} \leftarrow \emptyset$
           
           \FOR{$v \in \text{SortByDistance}(\mathcal{C} \cup \mathcal{N}(u))$}
               \STATE $pruned \leftarrow \text{False}$
               \FOR{$n \in \mathcal{N}_{new}$}
                   \IF{$\alpha_u \cdot d(n, v) \le d(u, v)$}
                       \STATE $pruned \leftarrow \text{True}$; break
                   \ENDIF
               \ENDFOR
               \IF{not $pruned$ \AND $|\mathcal{N}_{new}| < R$}
                   \STATE $\mathcal{N}_{new}.\text{add}(v)$
               \ENDIF
           \ENDFOR
           \STATE $\mathcal{N}(u) \leftarrow \mathcal{N}_{new}$
       \ENDFOR
   \ENDFOR
\end{algorithmic}
\end{algorithm}

\paragraph{Phase 1: Geometric Calibration.}
Before modifying the graph topology, the system first performs a global analysis of the dataset geometry. We estimate the LID for every point and aggregate the population statistics ($\mu, \sigma$) defined in Section~\ref{subsec:mapping}. 
This phase ``freezes'' the geometric profile of the dataset. By pre-computing these statistics, we decouple the complexity estimation from the graph update loop, ensuring that the mapping function $\Phi$ remains stable and computationally efficient during the intensive edge-selection process.

\paragraph{Phase 2: Manifold-Consistent Refinement.}
The index construction follows an iterative refinement strategy. 
Let $\mathcal{N}(u)$ denote the set of neighbors for node $u$ in the graph $G$. 
In each iteration, the algorithm dynamically updates $\mathcal{N}(u)$ by:
\begin{enumerate}
    \item Queries the pre-computed geometric profile to determine the node-specific constraint $\alpha(u)$.
    \item Explores the graph to identify a candidate pool $\mathcal{C}$.
    \item Filters connections using the dynamic occlusion criterion. 
\end{enumerate}

\subsection{Theoretical Analysis}
\label{subsec:analysis}

We analyze the proposed method regarding its computational overhead and topological guarantees.

\paragraph{Computational Complexity.}

The MCGI construction adds a pre-processing step (Phase 1) but maintains the same asymptotic complexity as standard graph construction algorithms (Phase 2).
\begin{itemize}
    \item \textit{Calibration Overhead:} The LID estimation requires a $k$-nearest neighbor search for a subset of points. Assuming a sample size of $N$ and a fixed reference set size, this operation is bounded by $O(N \log N)$. The subsequent Z-score calculation and $\alpha$ mapping are linear scan operations, $O(N)$.
    \item \textit{Construction Cost:} The core refinement loop (Algorithm 1, Phase 2) has a complexity of $O(T \cdot N \cdot R \cdot \log L)$, where $T$ is the number of iterations and $R$ is the degree bound. The node-specific parameter $\alpha(u)$ is accessed via a constant-time lookup $O(1)$ during the edge pruning check.
\end{itemize}
Since the calibration is a non-iterative, one-time operation, it does not alter the asymptotic complexity class of the indexing pipeline. 
The total time complexity remains dominated by the iterative distance computations required for edge selection, ensuring that the method scales linearly with $N$ similar to baseline iterative algorithms.

\paragraph{Connectivity Preservation.}
A primary concern with dynamic pruning is whether strict constraints fragment the graph into disconnected components.
To address this, we observe that the strictest constraint applied by our algorithm (where $\alpha(u) \to 1.0$) corresponds to the definition of the Relative Neighborhood Graph (RNG).
It has been theoretically established that the RNG is a supergraph of the Euclidean Minimum Spanning Tree (EMST) for any set of points in general position~\cite{TOUSSAINT1980261}.  
Since the EMST is connected, the RNG is necessarily connected.
In our method, the pruning parameter satisfies $\alpha(u) \ge 1.0$ for all $u$. Let $E_{MCGI}$ be the edge set produced by our algorithm and $E_{RNG}$ be the edge set of the exact RNG. The condition implies:
\begin{equation}
    E_{RNG} \subseteq E_{MCGI}.
\end{equation}
Thus, provided that the greedy search sufficiently explores the candidate space, the resulting topology retains the connectivity guarantees of the RNG, ensuring that every node remains reachable from the entry point.

\section{Evaluation}
\label{sec:evaluation}

In this section, we evaluate the proposed MCGI algorithm against state-of-the-art graph-based ANNS methods. 
We aim to answer the following research questions:
\begin{enumerate}
    \item Does the manifold-consistent pruning strategy improve the Recall-QPS trade-off compared to static graph indices?
    \item Is the dynamic parameter $\alpha(u)$ superior to fixed global relaxation factors?
    \item How does the geometric calibration phase affect the total index construction time?
\end{enumerate}

\subsection{Experimental Setup}
\label{subsec:setup}

\paragraph{Datasets.}
We select three benchmark datasets with varying dimensionality and geometric properties to evaluate the robustness of our method.
The statistics are summarized in Table~\ref{tab:datasets}.
\begin{itemize}
    \item SIFT1M~\cite{jegou2011product}: A standard dataset of 128-dimensional SIFT vectors.
    \item GloVe-100~\cite{pennington2014glove}: A dataset of word embeddings known for its high intrinsic dimensionality and clustered distribution.
    \item Deep1M~\cite{babenko2016efficient}: A deep learning feature dataset representative of modern semantic search workloads.
\end{itemize}

\begin{table}[h]
    \centering
    \caption{Dataset Statistics. $N$ denotes the number of base vectors, and $D$ denotes the ambient dimensionality.}
    \label{tab:datasets}
    \begin{tabular}{lrrr}
        \toprule
        Dataset & $N$ & $D$ & Metric \\
        \midrule
        SIFT1M    & 1,000,000 & 128 & Euclidean \\
        GloVe-100 & 1,183,514 & 100 & Euclidean \\
        Deep1M    & 1,000,000 & 96  & Euclidean \\
        \bottomrule
    \end{tabular}
\end{table}

\paragraph{Baselines.}
We compare MCGI with the following industry-standard graph indexing algorithms:
\begin{itemize}
    \item \textit{HNSW:} A hierarchical graph index that uses a fixed heuristic pruning strategy equivalent to strict RNG ($\alpha=1$).
    \item \textit{Vamana (DiskANN):} A flat graph index that employs a relaxed pruning criteria (typically $\alpha=1.2$) to ensure graph connectivity.
    \item \textit{NSG:} A search graph constructed on top of a k-nearest neighbor graph approximation.
\end{itemize}

\paragraph{Implementation Details.}
All algorithms are implemented in C++ and compiled with GCC 9.4 using -O3 optimization. Experiments are conducted on a workstation with an Intel Xeon Gold 6226R CPU (2.90 GHz) and 256GB RAM. 
To ensure a fair comparison, all methods use the same distance computation kernels (SIMD AVX2) and graph degree bounds ($R=64$ for construction, $R_{search}=32$).

\subsection{Performance Comparison}
\label{subsec:performance}

We report the search performance in Table~\ref{tab:performance}. 
To provide a rigorous quantitative comparison, we measure the maximum Queries Per Second (QPS) achievable at high-recall regimes (Recall@10 $\ge$ 0.95 and $\ge$ 0.98), which are critical for production environments.

% TODO: Update analysis based on table data
On the geometry-complex GloVe-100 dataset, MCGI demonstrates a significant throughput advantage. 
As shown in Table~\ref{tab:performance}, at the stringent 0.98 recall level, MCGI achieves higher QPS compared to the baselines.
Standard methods like Vamana suffer from the fixed parameter dilemma: a large $\alpha$ introduces redundant edges in dense clusters (slowing down search), while a small $\alpha$ cuts off necessary long-range links. 
By dynamically adapting $\alpha$, MCGI maintains high connectivity in sparse regions while aggressively pruning in dense manifolds, resulting in a more efficient graph navigation.

\don{TODO}
\begin{table}[h]
    \centering
    \caption{Search Performance (QPS) at fixed Recall@10 targets. Higher is better. Bold indicates the best result.}
    \label{tab:performance}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l c c c c}
        \toprule
        & \multicolumn{2}{c}{\textbf{GloVe-100}} & \multicolumn{2}{c}{\textbf{SIFT1M}} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5}
        Method & Recall=0.95 & Recall=0.98 & Recall=0.95 & Recall=0.98 \\
        \midrule
        HNSW    & TBD & TBD & TBD & TBD \\
        Vamana  & TBD & TBD & TBD & TBD \\
        NSG     & TBD & TBD & TBD & TBD \\
        \midrule
        \textbf{MCGI (Ours)} & \textbf{TBD} & \textbf{TBD} & \textbf{TBD} & \textbf{TBD} \\
        \bottomrule
    \end{tabular}
    }
\end{table}

\subsection{Ablation Study}
\label{subsec:ablation}

To validate the effectiveness of the proposed mapping function, we compare MCGI against the Vamana baseline with fixed global $\alpha$ values ranging from 1.0 (Strict RNG) to 1.2 (Relaxed).
As shown in Table~\ref{tab:ablation}, we analyze if a single static $\alpha$ can match the dynamic approach.

\begin{table}[h]
    \centering
    \caption{Comparison of Static vs. Dynamic Pruning on GloVe-100 (Recall@10 = 0.95).}
    \label{tab:ablation}
    \begin{tabular}{lcc}
        \toprule
        Method & QPS & Avg. Hop Count \\
        \midrule
        Static ($\alpha=1.0$) & TBD & TBD \\
        Static ($\alpha=1.2$) & TBD & TBD \\
        \midrule
        MCGI (Dynamic)        & TBD & TBD \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Parameter Sensitivity}
\label{subsec:sensitivity}

We analyze the impact of the hyperparameter $\alpha_{\max}$ used in the mapping function $\Phi$. We fix the lower bound $\alpha_{\min}=1.0$ and vary the upper bound $\alpha_{\max}$ from 1.1 to 1.5. 
Table~\ref{tab:sensitivity} summarizes the performance impact on the GloVe-100 dataset.

% TODO: Update analysis based on table data
The results indicate that the method is relatively insensitive to the exact value of $\alpha_{\max}$ within the range [1.1, 1.3].
Performance peaks when $\alpha_{\max}$ provides sufficient relaxation for isotropic regions without compromising the strict pruning required in dense clusters. 
Setting $\alpha_{\max}$ too high (e.g., 1.5) may introduce excessive edges, slightly degrading QPS due to increased distance computations per hop.

\begin{table}[h]
    \centering
    \caption{Impact of varying $\alpha_{\max}$ on GloVe-100 search performance (QPS at fixed Recall targets).}
    \label{tab:sensitivity}
    \begin{tabular}{c c c}
        \toprule
        $\alpha_{\max}$ & QPS (Recall=0.95) & QPS (Recall=0.98) \\
        \midrule
        1.1 & TBD & TBD \\
        1.2 & TBD & TBD \\
        1.3 & TBD & TBD \\
        1.4 & TBD & TBD \\
        1.5 & TBD & TBD \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Indexing Efficiency}
\label{subsec:efficiency}

Finally, we verify the computational overhead introduced by the geometric calibration phase. 
Table~\ref{tab:construction_time} breaks down the indexing time.
The LID estimation and $\alpha$ mapping steps are expected to consume a minor fraction of the total construction time, confirming that the pre-computation overhead is acceptable.

\begin{table}[h]
    \centering
    \caption{Construction Time Breakdown (minutes) on Deep1M.}
    \label{tab:construction_time}
    \begin{tabular}{lrrr}
        \toprule
        Method & Calibration & Refinement & Total \\
        \midrule
        Vamana & TBD & TBD & TBD \\
        MCGI   & TBD & TBD & TBD \\
        \bottomrule
    \end{tabular}
\end{table}
\section{Conclusion}


\nocite{*}
\bibliography{ref}
\bibliographystyle{icml2026}

\end{document}